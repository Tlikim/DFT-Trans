# DFT-Trans
[中文版](./README_zh.md)  
This is the official implementation of paper "DFT-Trans: Pretraining Cost Reduction via Domain Mixing Tokens of Time and Frequency for NLP Tasks"  
## Pretrain Corpus
We are providing the corpus used to pretrain the model.  
[C4](https://huggingface.co/datasets/allenai/c4), [Wikipeidia](https://huggingface.co/datasets/wikipedia) and 
[BookCorpus](https://huggingface.co/datasets/bookcorpus): These datasets are used for Bert, Albert. We can obtained from [huggingface](https://huggingface.co/).  
[C4BookC](https://pan.baidu.com/s/1BxkE75I9iKkWppkTFBWj2w?pwd=5ls4): This dataset is used to pre-train the DFT-Trans model. Approximately 40 million sentences. Consists of part of the C4 dataset and the BookCorpus dataset used to train our model.  


